# 《这就是搜索引擎核心技术详解》相关

 - [搜索引擎及其技术架构](https://github.com/luobotiantang/JavaRelatedBooks/blob/master/md/ThisIsTheSearchEngineCoreTechnology.md)  
 
     - [搜索引擎的重要地位](#搜索引擎的重要地位)
 
     - [搜索引擎技术发展史](#搜索引擎技术发展史)
 
     - [搜索引擎的3个目标](#搜索引擎的3个目标)
 
     - [搜索引擎的3个核心问题](#搜索引擎的3个核心问题)
 
     - [搜索引擎的技术架构](#搜索引擎的技术架构)
 
     - [搜索引擎架构图](https://www.processon.com/view/link/5ed34f437d9c080702854f3f)
 
 - [网络爬虫](#网络爬虫)
 
     - [通用爬虫框架](#通用爬虫框架)
 
     - [优秀爬虫的特性](#优秀爬虫的特性)
 
     - [爬虫质量的评价标准](#爬虫质量的评价标准)
 
     - [抓取策略](#抓取策略)
 
     - [网页更新策略](#网页更新策略)
 
     - [暗网抓取](#暗网抓取)
 
     - [分布式爬虫](#分布式爬虫)
 
 - [搜索引擎索引](#搜索引擎索引)  
 
     - [索引基础](#索引基础)
     
     - [单词词典](#单词词典)
     
     - [倒排列表](#倒排列表)
     
     - [建立索引](#建立索引)
     
     - [动态索引](#动态索引)
     
     - [索引更新策略](#索引更新策略)
     
     - [查询处理](#查询处理)
     
     - [多字段索引](#多字段索引)
     
     - [短语查询](#短语查询)
     
     - [分布式索引](#分布式索引)
     
 - [索引压缩](#索引压缩)
 
     - [词典压缩](#词典压缩)
     
     - [倒排列表压缩算法](#倒排列表压缩算法)
     
     - [文档编号重排序](#文档编号重排序)
     
     - [静态索引裁剪](#静态索引裁剪)
     
 - [检索模型与搜索排序](#检索模型与搜索排序)
     
     - [布尔模型](#布尔模型)
     
     - [向量空间模型](#向量空间模型)
     
     - [概率检索模型](#概率检索模型)
     
     - [语言模型方法](#语言模型方法)
     
     - [机器学习排序](#机器学习排序)
     
     - [检索质量评价标准](#检索质量评价标准)
     
 - [链接分析](#链接分析)  
  
     - [Web图](#Web图) 
     
     - [两个概念模型及算法之间的关系](#两个概念模型及算法之间的关系)
     
 
 ### 搜索引擎的重要地位
     由于目前互联网技术的爆炸性增长，信息过载的问题就目前来说越来越严重，由于互联网个性化的发展趋势越逐步展现，普通用户发布信息的成本
     越来越低，这个问题将会更加严重。这就是搜索引擎越来越重要的基础背景，搜索是目前解决信息过载的相对有效的方式，在没有更有效的替代解
     决方式出来之前，搜索引擎作为互联网网站和应用的入口及处于行业制高点的重要地位只会增强。
     
 ### 搜索引擎技术发展史
     1、分类目录
        采取分类目录的方式，纯人工方式并未采取什么高深的技术。如Yahoo、hao123这个时代的代表。
     2、文本检索
        采用经典的信息检索模型，采用布尔模型、向量空间模型或者概率模型，来查询用户的关键词和网页文本内容的相关程度。但是并未用到网页
        之间丰富的链接关系。早起的搜索引擎如AltaVista、Excite等大都采取这种模式。
     3、链接分析
        充分利用网页的链接关系，并深度挖掘和利用了网页链接所代表的含义。通常而言网页链接代表了一种推荐关系，所以通过链接分析可以在海
        量内容中找到重要的网页。如Google率先提出并使用PageRank链接分析技术。
     4、用户中心
        目前的搜索引擎大都可以归入第三代，即以理解用户需求为核心。
        目前搜索引擎大都致力于解决如下问题：如何能够理解用户发出的某个很短的查询词背后包含的真正需求，所以这一代的搜索引擎称之为以用
                                       户为中心的一代。
 ### 搜索引擎的3个目标
     应用形式：用户输入查询词，搜索引擎返回搜索结果。 
     1、更全
        索引的网页数量。可以通过提高网络爬虫相关技术来达到此目标。
     2、更快
        索引相关技术、缓存等技术的提出都是直接为了达到此目的。                                   
     3、更准
        如何使得搜索结果"更准"是最为关键的目标。(排序技术、链接分析技术、用户研究) 
 ### 搜索引擎的3个核心问题
     搜索引擎如何能够搜得更准是其最重要的目标。如何搜得更准涉及3个核心问题。
     1、用户真正的需求是什么
     2、哪些信息是和用户需求真正相关的
     3、哪些信息是用户可以依赖的      
 ### 搜索引擎的技术架构
     作为互联网应用中最具技术含量的应用之一，优秀的搜索引擎需要复杂的架构和算法，以此来支撑对海量数据的获取、存储，以即对用户查询的快
     速而准确的响应。
 ### 网络爬虫
     网络爬虫的作用
         高效的下载系统，将目前网页数以百亿计的网页数据传送到本地，在本地形成互联网网页的镜像备份。
 ### 通用爬虫框架
     首先从互联网页面中精心选择一部分网页，以这些网页的链接地址作为种子URL，将这些种子URL放入待抓取URL队列中，爬虫从待抓取URL队列依
     次读取，并将URL通过DNS解析，把链接地址转换为网站服务器对应的IP地址。然后将其和网页相对路径名称交给网页下载器，网页下载器负责页
     面内容的下载。对于下载到本地的网页，一方面将其存储到页面库中，等待建立索引等后续处理；另一方面将下载过的网页URL放入已抓取URL队
     列中，这个队列记载了爬虫系统已经下载过的网页URL，以避免网页的重复抓取。对于刚下载的网页，从中抽取所包含的所有链接信息，并在已抓
     取URL队列中检查，如果发现链接还没有被抓取过，则将这个URL放入待抓取URL队列末尾，在之后的抓取调度中会下载这个URL对应的网页。如此
     这般，形成循环，直到待抓取URL队列为空，这代表着爬虫系统已将能够抓取的网页尽数抓完，此时形成了一轮完整的抓取过程。
     爬虫划分以下3种类型
        1、批量型爬虫(Batch Crawler)
           有明确的抓取范围和目标
        2、增量型爬虫(Incremental Crawler)
           更新已有网页
        3、垂直型爬虫(Focused Crawler)
           关注特定主题内容或者属于特定行业的网页
 ### 优秀爬虫的特性
     1、高性能
        爬虫下载网页的抓取速度，常见的评价方式是以爬虫每秒能够下载的网页数量作为性能指标。
     2、可扩展性
        爬虫需要抓取网页数量巨大，即使单个爬虫的性能很高，要将所有网页都下载到本地，仍然需要相当长的时间周期，为了尽可能的缩短时间
        周期，爬虫系统应该有很好的可扩展性，即很容易通过增加抓取服务器和爬虫数量来达到此目的。
        分布式，增加并发性。
     3、健壮性
        服务器宕机，能够恢复之前抓取的内容和数据结构
     4、友好性
        1、保护网站的部分私密性
           A：爬虫禁抓协议
           B：网页禁抓标记
        2、减少抓取网站的网络负载 
           爬虫访问网站频率过高，会给服务器造成很大的访问压力，类似DOS攻击的效果。 
 ### 爬虫质量的评价标准
     1、抓取网页覆盖率
     2、抓取网页时新性
     3、抓取网页重要性
     决定了爬虫系统的质量和性能
        A：抓取策略
        B：网页更新策略
        C：暗网抓取
        D：分布式爬虫
     Google的两套爬虫系统
        A：Fresh Bot(以秒为更新周期)
        B：Deep Crawl Bot(以天为更新周期)           
 ### 抓取策略
     宽度优先遍历策略:
         将新下载网页追加到待抓取URL队列末尾。
     非完全PageRank策略:
         PageRank是一种链接分析算法，可以用来衡量网页的重要性.可以用PageRank的思想来对URL优先级进行排序，但这里有个问题，
         PageRank是个全局性算法，也就是说当所有网页都下载完成后，其计算结果才是可靠的，而爬虫的目的是下载网页，在运行中只能看到
         一部分页面，所以在抓取阶段的网页是无法获得可靠的PageRank得分的。
         对于已经下载的网页，加上待抓取URL队列中的URL一起，形成网页集合，在此集合内进行PageRank计算，计算完成后，将带抓取URL队
         列里的网页按照PageRank得分由高到低排序，形成的序列就是爬虫接下来应该依次抓取的URL列表，这也是为何称之为"非完全PageRank"
         的原因。
         如果每次新抓取到一个网页，就将所有已经下载的网页重新计算新的非完全PageRank值，明显效率太低，在现实中是不可行的。一个折中
         的办法是：每当新下载的网页攒够K个，将所有下载页面重新计算一遍新的非完全PageRank,这样的效率还勉强可以接受。
         但是又引来了新的问题：在展开下一轮PageRank计算之前，从新下载的网页抽取出包含的链接，很有可能这些链接的重要性非常高，理应
         优先下载这种情况该如何解决？非完全PageRank赋予这些新抽取出来但是又没有PageRank值的网页一个临时PageRank值，将这个网页的
         所有入链传导的PageRank值汇总，作为临时PageRank值，如果这个值比待抓取URL队列中已经计算出来的PageRank值的网页高，那么就
         优先下载这个URL。
     OCIP策略:
         Online Page Importance Computation,在线页面重要性计算，可以将其看作是一种改进的PageRank算法。在算法开始之前，每个互
         联网页面都赋予相同的"现金"cash，每当下载了某个页面P后，P将自己拥有的"现金"平均分配给页面中包含的链接页面，把自己的"现金"
         清空。而对于待抓取URL队列中的网页，则根据手头拥有的现金金额多少排序，优先下载现金最充裕的网页。OCIP从大的框架上与PageRank
         思路基本一致，区别在于：PageRank每次需要迭代计算，而OCIP策略不需要迭代过程，所以计算速度远远快于PageRank，适合实时计算
         使用。
     大站优先策略: 
         以网站为单位来衡量网页的重要性，对于待抓取URL队列中的网页，根据所属网站归类，其本质思想倾向于优先下载大型网站。
 ### 网页更新策略
     对于已经抓取的网页还要保持其内容和互联网页面内容的同步(可能抓取的内容已经删除了)，这取决于爬虫所采取的网页更新策略。
     历史参考策略:
         过去频繁更新的网页，那么将来也会频繁更新。所以，为了预估某个网页何时进行更新，可以参考其历史更新情况来做出决定。
         这种方法往往利用泊松过程来对网页的变化进行建模，根据每个网页过去的变动情况，利用模型预测将来何时内容再次发生变化。
         注：抓取策略应该忽略掉广告栏或者导航栏这种不重要的区域的频繁变化，而集中在主题内容的变化探测和建模上。
     用户体验策略:
         根据用户搜索排名进行更新
     聚类抽样策略:
         上面两种策略严重依赖网页的历史更新信息，因为这是能够进行后续计算的基础。但是现实中，为每个网页保存历史信息，会使搜索系统
         增加额外负担。但是如果没有历史信息就无法按照这两种思路去预估其更新周期。聚类抽样策略即是为了解决上述缺点而提出的。
         聚类抽样策略认为：网页具有一些属性，根据这些属性可以预测其更新周期，具有相似属性的网页，其更新周期也是类似的。
         能够体现网页更新周期的属性特征划分为两大类：静态特征和动态特征。
         静态特征：页面的内容、图片数量、页面大小、链接深度、PageRank值等十几种。
         动态特征：体现了静态特征随着时间的变化情况，比如图片数量的变化情况，入链出链的变化情况。
         聚类抽样策略效果好于前两种更新策略，但是对以亿计的网页进行聚类，其难度也是非常巨大的。
 ### 暗网抓取  
     Deep Web Crawling   
     所谓暗网：指目前搜索引擎爬虫按照常规方式很难抓取到的互联网页面。          
     如前所述，搜索引擎爬虫依赖页面中的链接关系发现新的页面，但是很多网站的内容是以数据库方式存储的，典型的例子就是一些垂直领域的
     网站，比如携程旅行网的机票数据，很难有显示链接指向数据库的记录，往往是用户输入查询之后来获得相关数据。
     为了能够对暗网数据进行索引，需要研发与常规爬虫机制不同的系统，这类爬虫被称作暗网爬虫。
     目的：将暗网数据从数据库中挖掘出来，并将其加入搜索引擎的索引，这样用户在搜索时便可利用这些数据，增加信息覆盖程度。
     因为关系到索引量的大小，目前大型搜索引擎服务提供商都将暗网挖掘作为重要的研究方向，Google、百度的"阿拉丁计划"。
     难点：
        1、查询组合太多，如果一一组合遍历，那么会给被访问网站造成巨大压力，所以如何精心组合查询选项是个难点。
           解决：富含信息查询模版(Informative Query Templates)
                ISIT算法：首先从一维模版开始，对一维查询模版逐个考察，看其是否富含信息查询模版，如果是的话，则将这个一维模版
                扩展到二维，再依次考察对应的二维模版，如此类推，逐个增加纬度，直到再也无法找到富含信息查询模版为止。
                Google的评测结果证明，这种方法和完全组合方式比，能够大幅度提升系统效率。
                数据挖掘：Google提出的算法和数据挖掘里经典的Apriori规则挖掘算法。
        2、有的查询是文本框，比如图书搜索中需要输入书名，爬虫怎样能够填入合适的内容，也颇具有挑战性。
           解决：对于输入中的文本框，需要爬虫自动生成查询。
                通过人工观察网站进行定位，提供一个与网站内容相关的初始种子查询关键词表，对于不同的网站，需要人工提供不同的词表，
                以此作为爬虫能够继续工作的基础条件，通过这种人工启发组合递归迭代的方式，尽可能覆盖数据库里的记录。
 ### 分布式爬虫       
     面对海量待抓取网页，只有采用分布式架构，才有可能在较短的时间内完成一轮抓取工作。
     分布式爬虫可以分为若干个分布式层级，不同的应用可能由其中部分层级构成。
     如：分布式爬虫的3个层级：
                        1、分布式数据中心；2、分布式抓取服务器；3、分布式爬虫程序
     常见的分布式架构有两种：
         1:住从式分布爬虫(Master-Slave)
                                        ———URL———>抓取服务器<——————
                                        |                        |
                                        |                        |
            待抓取URL队列---->URL服务器------URL--->抓取服务器<——————————互联网
                                        |                        |
                                        |                        |
                                        ———URL———>抓取服务器<——————
            其中有一台专门负责对其他服务器提供URL分发服务，其他机器则进行实际的网页下载。
            URL服务器维护待抓取URL队列，并从中获得待抓取网页的URL，分配给不同的抓取服务器，另外还要对抓取服务器之间的工作进行
            负载均衡，使得各个服务器承担的工作量大致相等，不至于出现忙的过忙、闲的过闲的情形。抓取服务器之间没有通信联系，每个
            抓取服务器只和URL服务器进行消息传递。
            Google在早期即采用此中主从分布式爬虫，但是此架构URL服务器承担更多的管理任务，同时待抓取URL队列数量巨大，所以URL
            服务器容易成为整个系统的瓶颈。                    
         2:对等式分布爬虫
            在对等式分布式爬虫体系中，服务器之间不存在分工差异，每台服务器承担相同的功能，各自负担一部分URL的抓取工作，Mercator
            爬虫采用此种体系结构。 
                                        ———URL———>抓取服务器<——————
                                        |                        |
                                        |                        |
                    待抓取URL队列-------Hash--URL----->抓取服务器<——————————互联网
                                        |                        |
                                        |                        |
                                        ———URL———>抓取服务器<——————
                    对待抓取URL队列中的URL进行Has取模(hash[URL]%m)。
                    注：m指抓取服务器个数。
                    缺点：抓取过程中某台服务器宕机，或者此时新加入一台抓取服务器，因为取模时m是以服务器个数确定的，所以此时m值
                         发生变化，导致大部分URL哈希取模后的值跟着变化，这意味着几乎所有任务都需要重新进行分配，无疑会导致资源
                         的极大浪费。
                    解决：为了解决哈希取模的对等式分布式爬虫存在的问题，UbiCrawler爬虫提出了改进方案，即放弃哈希取模方式，转而
                         采用一致性哈希方法(Consisting Hash)来确定服务器的任务分工。   
                         一致性哈希将网站的主域名进行哈希，映射为一个范围在0到2^32之间的某个数值，大量的网站主域名会均匀地哈希
                         到这个数值区间。
 ### 搜索引擎索引
     索引好比书籍的目录，目的是为了让人们更快的找到相关章节内容。                        
     如：hao123导航网站本质上也是互联网页面中的索引结构。
     在计算机科学领域，索引也是非常常用的数据结构。其根本目的是为了在具体应用中加快查找速度。
     具体到搜索引擎，索引更是其中最重要的核心技术之一，面对海量的网页内容，如何快速找到包含用户查询词的所有网页？倒排索引在其中扮
     演了关键的角色。
 ### 索引基础
     单词-——文档矩阵：
        搜索引擎的索引其实就是实现单词————文档矩阵的具体数据结构。
        可以有不同的方式实现上述概念模型，比如倒排索引、签名文件、后缀树等方式。但是各项实验数据表明，倒排索引是单词到文档映射关系
        的最佳实现方式。
     倒排索引基本概念：
        倒排索引用到的专用术语：
           文档：一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛些，代表以文本形式存在的存储对象。
                例如：Word、PDF、html、XML、一封邮件、一条短信、一条微博。
           文档集合：由若干个文档构成的集合称为文档集合。比如海量的互联网网页或者说大量的电子邮件。
           文档编号：文档的唯一标识，DocID.
           单词编号：单词的唯一表征。
           倒排索引：倒排索引是实现单词————文档矩阵的一种具体存储形式。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表
                    倒排索引主要由两部分组成：单词词典和倒排文件。
           单词词典：搜索引擎通常的索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串的集合，单词词典内每条索引
                    项记载单词本身的一些信息及指向倒排列表的指针。
           倒排列表：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项
                   （Posting）。根据倒排列表，即可获知哪些文档包含某个单词。              
           倒排文件：倒排单词的倒排列表所在磁盘文件。倒排文件是存储倒排索引的物理文件。
            __________________________________________________________
           |                                                          |
           |   内存中     词典————>单词1   单词2   单词3   单词4   单词5  |
           |_______________________|_______|__________________________|  
                                   |
                              词典指向倒排列表
            _______________________|___________________________________
           |                                                          |
           |                           倒排列表                        |
           |              倒排文件                                     |
           |   磁盘中                                                  |
           |                                                          |   
           |__________________________________________________________|
           
     倒排索引简单实例：
        文档编号DocID————————文档内容
            单词ID——————————单词————————倒排列表(DocID)
            单词ID——————————单词————————倒排列表(DocID;TF)
            单词ID——————————单词————————文档频率——————倒排列表(DocID;TF;<POS>)
            注：TF单词在文档出现的频率；POS单词在文档中出现的位置
 ### 单词词典
     单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记录某个单词对应的倒排列表在
     倒排文件中的位置信息。    
     数据结构影响搜索时的响应速度，常用的数据结构包括哈希加链表结构和树形词典结构。
     哈希加链表：
           哈希表——————————————>冲突表(单词1，单词2)
     树形结构：
           B树/B+树
           需要字典项能够按照大小排序(数字或者字符序)；
           中间节点用于指出一定范围内的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用；叶子节点存储单词的地址信
           息，根据这个地址就可以取出单词字符串。
 ### 倒排列表    
     Posting List
     倒排列表用来记录有哪些文档包含了某个单词。
     一般在文档集合里会有很多文档包含某个单词，每个文档会记录文档编号（DocID），单词在这个文档中出现的次数（TF）及单词在文档中哪
     些位置出现过等信息，这样与一个文档相关的信息被称做倒排索引项（Posting），包含这个单词的一系列倒排索引项形成了列表结构，这就
     是某个单词对应的倒排列表。
     在实际的搜索引擎中，并不存储倒排索引项中的实际文档编号，而是代之以文档编号差值(D-Gap)。
     之所以要对文档编号进行差值计算，主要原因是为了更好的对数据进行压缩，原始文档编号一般都是大数据，通过差值计算，就有效的将大数
     值转换为小数值，而这有助于增加数据的压缩率。
 ### 建立索引
     索引结构建立好了可以提高搜索的速度。    
     建立索引的方法：
         两遍文档遍历法(2-Pass In-Memory Inversion): 
             此方法需要对文档集合进行两遍扫描。
         排序法(Sort-based Inversion):
             两遍遍历法在建立索引的过程中，对内存的消耗要求较高，不同的文档集合包含文档数量大小不同，其所需内存大小是不确定的。
             当文档集合非常大时，可能因内存不够，导致无法建立索引。
             排序法对此做出了改进，该方法在建立索引的过程中，始终在内存中分配固定大小的空间，用来存放词典信息和索引的中间结果，
             当分配的空间被消耗光的时候，把中间结果写入磁盘，清空内存里中间结果所占空间，以用做下一轮存放索引中间结果的存储区。 
         归并法(Merge-based Inversion):
             排序法分配固定大小内存来建立索引，所以无论要建立索引的文档集合有多大，都可以通过这种方法完成。但是如上所述，在分
             配的内存定额被消耗光时，排序法只是将中间结果写入磁盘，而词典信息一直在内存中进行维护，随着处理的文档越来越多，词
             典里包含的词典项越来越多，所以占用内存越来越大，导致后期中间结果可用内存越来越少。归并法对此做出了改进，即每次将
             内存中数据写入磁盘时，包括词典在内的所有中间结果信息都被写入磁盘，这样内存所有内容都可以被清空，后续建立索引可以
             使用全部的定额内存。
 ### 动态索引
     在真实环境中，搜索引擎需要处理的文档集合往往是动态集合，即在建好初始的索引后，后续不断有新文档进入系统，同时原先的文档集合
     内有些文档可能被删除或者内容被更改。
 ### 索引更新策略
     动态索引通过在内存中维护临时索引，可以实现对动态文档和实时搜索的支持。但是服务器内存总是有限的，随着新加入系统的文档越来越多
     ，临时索引消耗的内存也会随之增加。当最初分配的内存被使用完时，要考虑将临时索引的内容更新到磁盘索引中，以释放内存空间容纳后续
     的新进文档，此时要考虑合理有效的索引更新策略。
     常用的索引更新策略：
        完全重建策略：
           完全重建策略是一个相当直观的方法，当新增文档达到一定数量，将新增文档和原先的老文档进行合并，然后利用前述章节提到的建立
           索引的方式，对所有文档重新建立索引。新索引建立完成后，老的索引被遗弃释放，之后对用户查询的响应完全由新的索引负责。
           因为重建索引需要较长时间，在进行索引重建的过程中，内存中仍然需要维护老的索引，来对用户的查询做出响应。只有当新索引完全
           建立完成后，才能释放旧的索引，将用户查询响应切换到新索引上。这种重建策略比较适合小文档集合，因为完全重建索引的代价较高，
           但是目前主流商业搜索引擎一般是采用此方式来维护索引的更新的，这与互联网本身的特性有关。
        再合并策略(Re-Merge)：
           有新增文档进入搜索系统时，搜索系统在内存维护临时到排索引来记录其信息，当新文档达到一定数量，或者制定大小的内存被消耗完
           则把临时索引和老文档的倒排索引进行合并,以生成新的索引。
           再合并策略是效率非常高的一种索引更新策略，主要原因在于：在对老的倒排索引进行遍历时，因为已经按照索引单词的词典序由低到
           高排好顺序，所以可以顺序读取文件内容，减少磁盘寻道时间，这是其高效的根本原因。但是这种方法也有其缺点，因为要生成新的倒
           排索引文件，所以对于老索引中的很多单词来说，尽管其倒排列表并未发生任何变化，但是也需要将其从老索引中读取出来并写入新索
           引中，这种对磁盘输入输出的消耗是没有太大必要且非常耗时的。
        原地更新策略：
           原地更新策略在索引合并时，并不生成新的索引文件，而是直接在原先老的索引文件里进行追加操作。
           将增量索引里单词的倒排列表项追加到老索引相应位置的末尾，这样就可达到上述目标，即只更新增量索引里出现的单词相关信息，其
           他单词相关信息不做变动。
           但是这里存在一个问题：
               对于倒排文件中的两个相邻单词，为了在查询时加快读取速度，其倒排列表一般是顺序序列存储的，这导致没有空余位置用来追加
               新信息。为了能够支持追加操作，原地更新策略在初始建立的索引中，会在每个单词的倒排列表末尾预留出一定的磁盘空间，这样
               ，在进行索引合并时，可以将增量索引追加到预留空间中。
           原地更新策略的出发点很好，但是实验数据证明其索引更新效率比再合并策略低，主要是出于以下两个原因。
               1、在这种方法中，对倒排列表进行“迁移”是比较常见的操作，为了能够进行快速迁移，需要找到足够大的磁盘连续存储区，所以
                  这个策略需要对磁盘可用空间进行维护和管理，而这种维护和查找成本非常高，这成为该方法效率的一个瓶颈。
               2、对于倒排文件中的相邻索引单词，其倒排列表顺序一般是按照相邻单词的词典序存储的，但是由于原地更新策略对单词的倒排
                  列表做数据迁移，某些单词及其对应倒排列表会从老索引中移出，这样就破坏了这种单词连续性，导致在进行索引合并时不能
                  进行顺序读取，必须维护一个单词到其倒排文件相应位置的映射表，而这样做，一方面降低了磁盘读取速度，另外一方面需要
                  大量的内存来存储这种映射信息。      
        混合策略：
           混合策略的出发点是能够结合不同索引更新策略的长处，将不同的索引更新策略混合，以形成更高效的方法。    
           混合策略一般会将单词根据其不同性质进行分类，不同类别的单词，对其索引采取不同的索引更新策略。常见的做法是：根据单词的倒
           排列表长度进行区分，因为有些单词经常在不同文档中出现，所以其对应的倒排列表较长，而有些单词很少见，则其倒排列表就较短。
           根据这一性质将单词划分为长倒排列表单词和短倒排列表单词。长倒排列表单词采取原地更新策略，而短倒排列表单词则采取再合并策
           略。
           之所以这样做，是由于原地更新策略更适合长倒排列表单词，因为这种策略能够节省磁盘读/写次数，而长倒排列表单词的读/写开销明
           显要比短倒排列表单词大很多，所以如果采用原地更新策略，效果体现得比较显著。而大量短倒排列表单词读/写开销相对而言不算太大，
           所以利用再合并策略来处理，则其顺序读/写优势也能被充分利用。
 ### 查询处理
     搜索引擎构建索引的目的是更快速的提取与用户查询相关的文档信息，假设搜索引擎已经建立好了索引，如何利用倒排索引来影响用户的查询，
     查询处理的意思就是主要描述搜索引擎对于用户查询的处理过程。    
     常见的查询处理机制：
        一次一文档方式：
              所谓的一次一文档，就是以倒排列表中包含的文档为单位，每次将其中某个文档与查询的最终相似性得分计算完毕，然后开始计算
              另外一个文档的最终得分，直到所有文档的得分都计算完毕为止。
        一次一单词方式 
        跳跃指针
 ### 多字段索引
     例如发送邮件的搜索：标题、摘要、正文
     实现多字段索引的3种方式：
        多索引方式
           针对不同的字段，分别建立一个索引，当用户指定某个字段作为搜索范围时，可以从响应的索引里提取结果。
        倒排列表方式
        扩展列表方式      
           扩展列表是实际中应用得比较多的支持多字段索引的方法。这个方法为每个字段建立一个列表，这个列表记载了每个文档这个字段对应
           的出现位置信息。
 ### 短语查询
     较常见的支持短语查询到的技术方法包括：
         位置信息索引
            文档ID、单词频率、单词位置
         双词索引
            首词+下词
            缺点：词典纬度由一维变二维
         短语索引
            之前介绍的常规索引结构中，词典都是以单词作为查询和存储单位的，如果将其扩展，在词典中直接加入多词短语并维护短语的倒排
            列表，这样也可以对短语进行支持。
            缺点：不可能事先将所有短语都建好索引。通用的做法是挖掘出热门短语，为这些短语专门建立索引，对于其他的短语查询，则采用
                 常规方法处理。
            数据来源：用户查询日志(Search Log)
     也可以将三者组合
         不同的短语索引结构有其各自的特点，位置索引适合处理常规的短语查询，即计算代价较小的短语，双词索引适合处理计算代价较高的短
         语查询，而短语索引则适合处理热门短语查询或者文本中高频度出现的短语，此三者是以互补关系存在的，如果能够在构建系统时有机集
         成三者，就能使系统效率发挥出综合优势。
 ### 分布式索引
     搜索引擎需要处理的文档集合数量非常庞大时，靠单机往往难以承担如此重任，此时需要考虑分布式解决方案，即每台机器维护整个索引的一
     部分，由多台机器协作来完成索引的建立和查询的响应。
     目前常用的分布式索引：
         按文档对索引划分
         按单词对索引划分
     两种方案的比较：按文档来对索引进行划分是比较常用的，而按单词进行索引划分只在比较特殊的应用场合才使用。之所以如此，是因为按单
                   词进行索引划分在以下几个方面存在不足。
                   可扩展性、负载均衡、容错性、对查询处理方式的支持
 ### 索引压缩
     对于海量网页数据，为其建立倒排索引往往需要耗费较大的磁盘空间，尤其是一些比较常见的单词，其对应的倒排列表可能大小有几百兆。如
     果搜索引擎在响应用户查询的时候，用户查询中包含常见词汇，就需要将大量的倒排列表信息从磁盘读入内存，之后进行查询处理给出搜索结
     果。由于磁盘读/写速度往往是个瓶颈，所以包含常用词的用户查询，其响应速度会受到严重影响。索引压缩则可以利用数据压缩算法，有效
     地将数据量减少，这样一方面可以减少索引占用的磁盘空间资源，另一方面可以减少磁盘读/写数据量，加快用户查询的响应速度。                  
     倒排索引组成：单词词典、单词对应的倒排列表
     所以索引的压缩算法：针对词典的压缩、针对倒排列表的压缩
     倒排索引压缩：
        无损压缩
           就是将原始倒排列表数据量减小，但是信息并不会因为占用空间的减小而有所损失，通过解压缩算法可以完全恢复原始信息。
        有损压缩
           而有损压缩则是通过损失部分不重要的信息，以此来获得更高的数据压缩率。
 ### 词典压缩
     为了快速响应用户查询，词典数据往往会全部加载到内存中，以加快查找速度。 
 ### 倒排列表压缩算法
     单词对应的倒排列表一般记载3类信息：文本编号、词频信息、单词位置序列信息。
     评价索引压缩算法的指标：压缩率、压缩速度、解压速度(关系用户体验)
     一元编码与二进制编码：所有倒排列表压缩算法的基本组成元素，不论压缩算法内部逻辑思路是怎样的，最终都要以这两种格式来对数据进行
     表示。
     Elias Gamma算法与Elias Delta算法
     Golomb算法与Rice算法
     变长字节算法(Variable Byte)
     SimpleX系列算法
     PForDelta算法
        PForDelta压缩算法是目前解压速度最快的一种倒排文件压缩算法，其基本出发点是：尽可能一次性压缩和解压多个数值。尽管SimpleX
        系列算法也利用了这一点，但PForDelta算法在这方面做得更好。
 ### 文档编号重排序
     DocID Reordering
     对于索引压缩技术来说，如果需要压缩的原始数据的值越小，压缩效果越好，之前介绍的倒排列表中文档ID采用D-Gap（文档差值编号）进行
     编码就是出于这种考虑。 
 ### 静态索引裁剪
     Ststic Index Pruning
     本章前述压缩算法都属于无损压缩，即数据在压缩前后没有任何丢失，本节要讲述的静态索引裁剪则属于有损压缩，通过主动抛弃一部分不重
     要的信息来达到更好的数据压缩效果。 
     静态索引裁剪的出发点是基于如下考虑：对于用户查询来说，搜索系统往往只需要返回相关性最高的K个网页，不需要将所有相关网页都呈现
     给用户。既然如此，对于词典中的某个单词，其对应的倒排列表中的索引项，对计算网页和用户查询最终相关性得分的贡献是不同的，有的对
     于计算最终得分很重要，有些则不是那么重要。所以可以将那些不重要的索引项从倒排索引中清除掉，只保留重要的索引项，这样就有效地减
     少了索引的大小，同时尽可能保证搜索质量，使得用户基本察觉不到索引项是不完整的。
     以单词为中心的索引裁剪
     以文档为中心的索引裁剪
 ### 检索模型与搜索排序
     搜索结果排序是搜索引擎最核心的组成部分，很大程度上决定了搜索引擎的质量好坏及用户接受与否。
     判断网页内容是否与用户查询相关，这依赖于搜索引擎所采取的检索模型。
     检索模型：
         布尔模型、向量空间模型、概率模型、语言模型、机器学习排序算法
 ### 布尔模型
     布尔模型是检索模型中最简单的一种，其数学基础是集合论。在布尔模型中，文档与用户查询由其包含的单词集合来表示，两者的相似性则通
     过布尔代数运算来进行判定。用户查询一般使用逻辑表达式，即使用“与/或/非”这些逻辑连接词将用户的查询词串联，以此作为用户的信息需
     求的表达。        
     缺点：搜索结果过于粗糙，不能单纯以相关或者不相关输出搜索结果。 
 ### 向量空间模型
     Vector Space Model
 ### 概率检索模型
     概率检索模型是目前效果最好的模型之一，在TREC等各种检索系统评测会议已经证明了这一点，而且okapi BM25这一经典概率模型计算
     公式已经在商业搜索引擎的网页排序中广泛使用。
 ### 语言模型方法
     从基本思路上来说，其他的大多数检索模型的思考路径是从查询到文档，即给定用户查询，如何找出相关的文档。语言模型方法的思路正好
     相反，是由文档到查询这个方向，即为每个文档建立不同的语言模型，判断由文档生成用户查询的可能性有多大，然后按照这种生成概率由
     高到低排序，作为搜索结果。
 ### 机器学习排序
     Learning to Rank
     机器学习排序系统由4个步骤组成：
         人工标注训练数据
         文档特征抽取
         学习分类函数
         在实际搜索系统中采用机器学习模型
 ### 检索质量评价标准        
     用精确率和召回率这两个指标来评价搜索质量。    
     精确率：就是本次搜索结果中相关文档所占的比例，分子为本次搜索结果中的相关文档，分母为本次搜索结果包含的所有文档，两者相除得
            到精确率。
     召回率：即本次搜索结果中包含的相关文档占整个集合中所有相关文档的比例。 
     常用的评估搜索引擎精度的指标有P@10和MAP。  
 ### 链接分析
     搜索引擎在查找能够满足用户请求的网页时，主要考虑两方面的因素：
        1、用户发出的查询与网页内容的内容相似性得分。
        2、通过链接分析方法计算获得的得分，即网页的重要性。 
 ### Web图
     互联网包含了海量网页，而网页和一般文本的一个重要区别是在页面内容中包含相互引用的链接（Link），如果将一个网页抽象成一个节点，
     而将网页之间的链接理解为一条有向边，则可以把整个互联网抽象为一个包含页面节点和节点之间联系边的有向图，称之为Web图。                           
 ### 两个概念模型及算法之间的关系
     随机游走模型(Random Surfer Model)   
     子集传播模型
                    
        
         
 
 
 
           
     
     
     
     
     
     
             

> reubenwang@foxmail.com
> 没事别找我，找我也不在！--我很忙🦆